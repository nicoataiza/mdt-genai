import os
import time
import sys
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize OpenAI Client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Prompts
PROMPT_GENERATE_MDT = """
Given the case study as a .pdf file, generate a plausible multi-disciplinary meeting transcript consisting of only the relevant specialists. There is no need to give names to each specialist, but instead refer to them by their specialty. The goal of the meeting is to eventually come up with the observations and conclusions made in the case study. Begin each meeting with the attendance of the specialists. Include a conclusion of the meeting the summarizes the general findings of the discussion. Return only the transcript with no other text.
"""

PROMPT_FACTUAL_CORRECTNESS = """
You will be given a case study in a .pdf file and a meeting transcript below. The meeting transcript is a discussion that should result in the creation of the case study. Be as strict as possible. Your task is to rate the transcript on one metric and provide appropriate feedback:

Evaluation Criteria: Factual Correctness (1-5): This is a measurement of how factually correct the generated transcript with respect to the case study. A higher score means that the transcript is completely factual, while a score of 0 means that the transcript contains multiple hallucinations not found in the case study.

Rubric: 
5 – Fully Accurate: Every factual detail in the transcript aligns precisely with the case study, with no additions, distortions, or omissions.
4 – Mostly Accurate: The transcript is largely factual with only minor discrepancies or one small omission/addition that does not alter overall understanding.
3 – Moderately Accurate: Several factual inconsistencies or omissions are present, but the core narrative remains recognizable and generally aligned with the case study.
2 – Minimally Accurate: Many factual errors, omissions, or added information not found in the case study significantly distort or misrepresent the original content.
1 – Factually Incorrect: The majority of the transcript does not reflect the case study; it includes numerous hallucinations or contradictions, making it largely unreliable.

Evaluation Steps:
1. Read the Case Study (PDF)
2. Read the Meeting Transcript
3. Compare for Factual Alignment
4. Rate on a Scale of 1 to 5 with 5 containing no discrepancies.
5. Write Feedback

Meeting Transcript:
{transcript}
"""

PROMPT_PLAUSIBILITY = """
You will be given a meeting transcript below. The meeting transcript is a discussion that should result in the creation of a case study. Be as strict as possible. However, ignore citation tokens in your evaluation. Your task is to rate the transcript on one metric and provide appropriate feedback: 

Evaluation Criteria: Plausibility (1-5): This is a measurement of how plausible the generated transcript is with respect to the case study. A higher score means that the transcript exhibits an actual conversation that human specialists made, while a score of 0 means that the transcript is most likely generated by a LLM - or conventionally, does not pass the Turing test.

5 – Highly Plausible: The transcript reads as a natural, unscripted expert conversation with realistic dynamics, clarifications, and occasional missteps; it fully resembles a real human meeting.
4 – Mostly Plausible: The transcript is generally believable with minor signs of artificiality (e.g., overly clean logic or lack of natural hesitations), but still feels like a real discussion among professionals.
3 – Moderately Plausible: The transcript includes plausible topic flow but has noticeable signs of artificial generation, such as perfectly sequential logic or repetitive phrasing, reducing believability.
2 – Barely Plausible: The transcript contains multiple unnatural elements—uniform tone, excessive agreement, or robotic transitions—that make it unlikely to be human conversation, though not entirely implausible.
1 – Not Plausible:The transcript clearly lacks realistic conversational structure, with mechanical responses, unrealistic jargon use, or a complete absence of human error or nuance.

Evaluation Steps: 
1. Read the Meeting Transcript
2. Identify Markers of Human Interaction vs. LLM Generation
3. Rate on a Scale of 1 to 5
4. Write Feedback

Meeting Transcript:
{transcript}
"""


def upload_file(file_path, mock_mode=False):
    """Uploads a file to OpenAI for use with assistants."""
    if mock_mode:
        print("--- MOCK MODE: Pretending to upload file ---")
        if not os.path.exists(file_path):
            print(f"Error: File not found at {file_path}")
            sys.exit(1)

        # Return a mock object that simulates the real OpenAI file object
        class MockFile:
            id = "fake_file_123"

        return MockFile()

    if not os.path.exists(file_path):
        print(f"Error: File not found at {file_path}")
        sys.exit(1)

    print(f"Uploading file: {file_path}...")
    file = client.files.create(file=open(file_path, "rb"), purpose="assistants")
    print(f"File uploaded. ID: {file.id}")
    return file


def create_assistant(mock_mode=False):
    """Creates an assistant with file search capabilities."""
    if mock_mode:
        print("--- MOCK MODE: Pretending to create assistant ---")

        # Return a mock object that simulates the real OpenAI assistant object
        class MockAssistant:
            id = "fake_assistant_123"

        return MockAssistant()

    print("Creating Assistant...")
    assistant = client.beta.assistants.create(
        name="MDT Generator & Evaluator",
        instructions="You are a helpful medical AI assistant capable of analyzing case studies and generating transcripts.",
        model="gpt-4o",
        tools=[{"type": "file_search"}],
    )
    print(f"Assistant created. ID: {assistant.id}")
    return assistant


def run_thread(assistant_id, thread_id, mock_mode=False, step="generate"):
    """Runs the thread and waits for completion."""
    if mock_mode:
        print(f"--- MOCK MODE: Pretending to run thread for step: {step} ---")
        time.sleep(1)
        if step == "generate":
            return "This is a mock transcript. In a real run, this would be a full multi-disciplinary team meeting transcript."
        elif step == "factual":
            return "This is mock feedback for factual correctness. Score: 5/5."
        elif step == "plausibility":
            return "This is mock feedback for plausibility. Score: 5/5."
        return "Mock response."

    run = client.beta.threads.runs.create(
        thread_id=thread_id, assistant_id=assistant_id
    )

    print(f"Run started (ID: {run.id}). Waiting for completion...")
    while True:
        run_status = client.beta.threads.runs.retrieve(
            thread_id=thread_id, run_id=run.id
        )
        if run_status.status == "completed":
            print("Run completed.")
            break
        elif run_status.status in ["failed", "cancelled", "expired"]:
            print(f"Run failed with status: {run_status.status}")
            sys.exit(1)
        time.sleep(2)

    messages = client.beta.threads.messages.list(thread_id=thread_id)
    # Return the latest message content
    return messages.data[0].content[0].text.value


def main():
    if len(sys.argv) < 2:
        print("Usage: python src/main.py <path_to_pdf> [--mock]")
        sys.exit(1)

    pdf_path = sys.argv[1]
    mock_mode = "--mock" in sys.argv

    if mock_mode:
        print("\n*** RUNNING IN MOCK MODE - NO API CALLS WILL BE MADE ***\n")

    # 1. Upload File
    file_obj = upload_file(pdf_path, mock_mode=mock_mode)

    # 2. Create Assistant
    assistant = create_assistant(mock_mode=mock_mode)

    # 3. Create Thread
    print("Creating Thread...")
    thread = (
        client.beta.threads.create()
        if not mock_mode
        else type("obj", (object,), {"id": "fake_thread_123"})()
    )

    try:
        # --- TASK 1: Generate MDT Transcript ---
        print("\n--- Task 1: Generating MDT Transcript ---")

        # Add message with file attachment
        client.beta.threads.messages.create(
            thread_id=thread.id,
            role="user",
            content=PROMPT_GENERATE_MDT,
            attachments=[{"file_id": file_obj.id, "tools": [{"type": "file_search"}]}],
        )

        transcript = run_thread(
            assistant.id, thread.id, mock_mode=mock_mode, step="generate"
        )
        print("\n[GENERATED TRANSCRIPT]\n")
        print(transcript)
        print("\n" + "=" * 50 + "\n")

        # --- TASK 2: Factual Correctness ---
        print("\n--- Task 2: Evaluating Factual Correctness ---")

        # We pass the transcript into the prompt. The file is already in the thread context.
        formatted_prompt_factual = PROMPT_FACTUAL_CORRECTNESS.format(
            transcript=transcript
        )

        client.beta.threads.messages.create(
            thread_id=thread.id, role="user", content=formatted_prompt_factual
        )

        factual_feedback = run_thread(
            assistant.id, thread.id, mock_mode=mock_mode, step="factual"
        )
        print("\n[FACTUAL CORRECTNESS FEEDBACK]\n")
        print(factual_feedback)
        print("\n" + "=" * 50 + "\n")

        # --- TASK 3: Plausibility ---
        print("\n--- Task 3: Evaluating Plausibility ---")

        formatted_prompt_plausibility = PROMPT_PLAUSIBILITY.format(
            transcript=transcript
        )

        client.beta.threads.messages.create(
            thread_id=thread.id, role="user", content=formatted_prompt_plausibility
        )

        plausibility_feedback = run_thread(
            assistant.id, thread.id, mock_mode=mock_mode, step="plausibility"
        )
        print("\n[PLAUSIBILITY FEEDBACK]\n")
        print(plausibility_feedback)
        print("\n" + "=" * 50 + "\n")

    finally:
        pass


if __name__ == "__main__":
    main()
